{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzZgvMx+BH+VikhSYDrmrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamkesava/Artificial-Intelligence/blob/main/textgenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense,Activation\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Load and preprocess the text data\n",
        "file_path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(file_path, 'rb').read().decode(encoding='utf-8').lower()\n",
        "text = text[300000:800000]\n",
        "\n",
        "# Create a mapping of unique characters to indices and vice versa\n",
        "characters = sorted(set(text))\n",
        "char_to_index = dict((c, i) for i, c in enumerate(characters))\n",
        "index_to_char = dict((i, c) for i, c in enumerate(characters))\n",
        "\n",
        "SEQ_LENGTH = 40\n",
        "STEP_SIZE = 3\n",
        "\n",
        "# Prepare the sentences and the next characters\n",
        "sentences = []\n",
        "next_characters = []\n",
        "for i in range(0, len(text) - SEQ_LENGTH, STEP_SIZE):\n",
        "    sentences.append(text[i:i + SEQ_LENGTH])\n",
        "    next_characters.append(text[i + SEQ_LENGTH])\n",
        "\n",
        "# Initialize the x and y arrays\n",
        "x = np.zeros((len(sentences), SEQ_LENGTH, len(characters)), dtype=bool)\n",
        "y = np.zeros((len(sentences), len(characters)), dtype=bool)\n",
        "\n",
        "# Fill the arrays with the data\n",
        "for i, sent in enumerate(sentences):\n",
        "    for t, char in enumerate(sent):\n",
        "        x[i, t, char_to_index[char]] = 1\n",
        "    y[i, char_to_index[next_characters[i]]] = 1\n",
        "\n",
        "model=Sequential()\n",
        "model.add(LSTM(128,input_shape=(SEQ_LENGTH,len(characters))))\n",
        "model.add(Dense(len(characters)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(lr=0.01))\n",
        "\n",
        "model.fit(x,y,batch_size=256,epochs=4)\n",
        "\n",
        "model.save('textgenerator.model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCLHrgy7a9Fy",
        "outputId": "f7595311-939a-4868-b822-9f60fbbb181c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "651/651 [==============================] - 119s 178ms/step - loss: 2.6834\n",
            "Epoch 2/4\n",
            "651/651 [==============================] - 116s 177ms/step - loss: 2.2959\n",
            "Epoch 3/4\n",
            "651/651 [==============================] - 116s 178ms/step - loss: 2.1808\n",
            "Epoch 4/4\n",
            "651/651 [==============================] - 115s 176ms/step - loss: 2.0971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to sample the next character based on the model's predictions\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "# Generate text function\n",
        "def generate_text(length, temperature=1.0):\n",
        "    start_index = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "    generated = ''\n",
        "    sentence = text[start_index:start_index + SEQ_LENGTH]\n",
        "    generated += sentence\n",
        "\n",
        "    for i in range(length):\n",
        "        x = np.zeros((1, SEQ_LENGTH, len(characters)))\n",
        "        for t, char in enumerate(sentence):\n",
        "            x[0, t, char_to_index[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x, verbose=0)[0]\n",
        "        next_index = sample(predictions, temperature)\n",
        "        next_char = index_to_char[next_index]\n",
        "\n",
        "        generated += next_char\n",
        "        sentence = sentence[1:] + next_char\n",
        "\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "JQOVqEnNlShT"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(length=400, temperature=0.2)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GRvDf0yhr2ZP",
        "outputId": "11d1e6be-78da-4d1f-d299-1ea0d39c27fd"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e so\n",
            "that hardly can i cheque my eyes fristher shat here she the and the beather and in the sead the beat,\n",
            "the that the pare the the have the sare the heard in the beather the forest the hearder the sofrenes,\n",
            "and the beather so sond the hore the parest the seat his the singer and and the beather,\n",
            "the that sond the hard the mere the beaten's my and i hath the beather,\n",
            "and in the seat the beather and the beather the bearser and beander so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generated_text = generate_text(length=400, temperature=0.3)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZZwg9twseMu",
        "outputId": "3c1fce96-00a1-4b87-e620-6b51696395cd"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comes from myself, it shall scarce boot is the beather and and thes fored wath then thou sand in the king the comen the ingerenower and in seanter and and danger in with this leands and the beate,\n",
            "and the to mand wather the hard the paresen\n",
            "the pareenter and hing me to the thee sond.\n",
            "\n",
            "ringer:\n",
            "i wert in the merenter and the sore the deat and the bearse the toreno bothen the with the ford and of the kend will stathering sis beadse,\n",
            "the th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(length=400, temperature=0.4)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVZe-P4gsw1X",
        "outputId": "0c9c0a3d-8b1f-4a3e-f025-2d7b55e019ae"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "riar, to know his remedy:\n",
            "if all else fare thit sead in my and for bearest his fare thou dather the heard and is for heres,\n",
            "for me the all me in the beanter sore to hing and your be the sored is is and noment hing and bearsender,\n",
            "and me the erofrome in the beate the merese for with the beat and the me is foren,\n",
            "and seare the ford the betthed in the peather seand and sillling hing to hanger parenest.\n",
            "\n",
            "lorde:\n",
            "i withy rome the to mour and \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(length=400, temperature=0.5)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9HzowGfs47v",
        "outputId": "4450923f-65e9-4740-8a6f-3af938403976"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " steep'd in blood? ah, what an unkind houll the ears ape\n",
            "theingret thes ence so not and save\n",
            "the beating the pare in thear mong.\n",
            "\n",
            "arding:\n",
            "seant hearst wo king for the thes porse the sile.\n",
            "\n",
            "thenre, it marke te king rowr shar to lead.\n",
            "\n",
            "our of hor in willoug longerenet:\n",
            "and in the s and bither meas the will there in frollone lecks,\n",
            "i hes ware mone the beates i hath end sor\n",
            "is men and grodselofnor\n",
            "the garing of the thing toun seon our share\n"
          ]
        }
      ]
    }
  ]
}